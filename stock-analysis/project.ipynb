<<<<<<< HEAD
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git # Stocks Analysis Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# MLRun imports\n",
    "from mlrun import mlconf\n",
    "import mlrun\n",
    "\n",
    "\n",
    "# Setup API Endpoint\n",
    "mlconf.dbpath = 'http://mlrun-api:8080'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MLRun stocks project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_project\n",
    "\n",
    "# update the dir and repo to reflect real locations \n",
    "# the remote git repo must be initialized in GitHub\n",
    "project_dir = os.path.abspath('./')\n",
    "remote_git = 'https://github.com/mlrun/demos.git'\n",
    "\n",
    "# Create the project\n",
    "project = new_project('stocks-test', project_dir, init_git=False)\n",
    "\n",
    "# We can update our project directory to the latest status by running\n",
    "# newproj.pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an artifact path to keep track of where our artifacts are going\n",
    "ARTIFACT_PATH =  os.path.join(os.path.abspath('./'), 'artifacts')\n",
    "mlconf.artifact_path = ARTIFACT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.serving.ServingRuntime at 0x7fa053823e50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set functions to project\n",
    "# project.set_function('code/00-train-sentiment-analysis-model.ipynb', name='bert_sentiment_classifier_trainer', kind='job')\n",
    "project.set_function('code/00-train-sentiment-analysis-model.ipynb', name='bert_sentiment_classifier_trainer')\n",
    "project.set_function('code/01-read-stocks.ipynb', name='stocks_reader')\n",
    "project.set_function('code/02-read-news.ipynb', name='news_reader')\n",
    "project.set_function('code/03-stream-viewer.ipynb', name='stream_viewer')\n",
    "\n",
    "mlrun.mlconf.hub_url = 'https://raw.githubusercontent.com/aviaIguazio/functions/development/sentiment_analysis_serving/function.yaml'\n",
    "project.set_function('hub://sentiment_analysis_serving', name='sentiment_analysis_server')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a pre-trained model (optional)\n",
    "Since running the [training](training/bert_sentiment_classification.ipynb) part to achieve good results may take some time, we had already trained and uploaded a model to a public location.  \n",
    "You can easily download it by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘/User/test/demos/stock-analysis/models/model.pt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this to download the pre-trained model to your `models` directory\n",
    "\n",
    "import os\n",
    "model_location = 'https://iguazio-sample-data.s3.amazonaws.com/models/model.pt'\n",
    "saved_models_directory = os.path.join(os.path.abspath('./'), 'models')\n",
    "\n",
    "# Create paths\n",
    "os.makedirs(saved_models_directory, exist_ok=1)\n",
    "model_filepath = os.path.join(saved_models_directory, os.path.basename(model_location))\n",
    "!wget -nc -P {saved_models_directory} {model_location} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.serving.states.TaskState at 0x7fa053876c90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add model \n",
    "project.func('sentiment_analysis_server').add_model(\"model1\", class_name='SentimentClassifierServing', model_path=model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a Grafana Dashboard  \n",
    "To track the different stocks on a live dashboard we will use **Grafana**.  <br>\n",
    "We will use [Grafwiz](https://github.com/v3io/grafwiz) to define and deploy the dashboard directly from this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/v3io/grafwiz\n",
      "  Cloning https://github.com/v3io/grafwiz to /tmp/pip-req-build-d32t72km\n",
      "Requirement already satisfied (use --upgrade to upgrade): grafwiz===-0.1.0- from git+https://github.com/v3io/grafwiz in /User/.pythonlibs/jupyter-avia/lib/python3.7/site-packages\n",
      "Collecting grafanalib==0.5.3\n",
      "  Using cached grafanalib-0.5.3-py3-none-any.whl (26 kB)\n",
      "Collecting attrs==19.1.0\n",
      "  Using cached attrs-19.1.0-py2.py3-none-any.whl (35 kB)\n",
      "Building wheels for collected packages: grafwiz\n",
      "  Building wheel for grafwiz (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for grafwiz: filename=grafwiz-_0.1.0_-py3-none-any.whl size=9302 sha256=cc9c54a9e191711aadb39ee9118a68d4e46535f517fda0d836ef75abda8108cf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cy_i0o4u/wheels/2f/6d/71/443942a9d87f91125fc43e7353667ca41489b2fa023fd0fc48\n",
      "Successfully built grafwiz\n",
      "Installing collected packages: attrs, grafanalib\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 20.3.0\n",
      "    Uninstalling attrs-20.3.0:\n",
      "      Successfully uninstalled attrs-20.3.0\n",
      "  Attempting uninstall: grafanalib\n",
      "    Found existing installation: grafanalib 0.5.9\n",
      "    Uninstalling grafanalib-0.5.9:\n",
      "      Successfully uninstalled grafanalib-0.5.9\n",
      "Successfully installed attrs-19.1.0 grafanalib-0.5.3\n",
      "Collecting grafanalib\n",
      "  Using cached grafanalib-0.5.9-py3-none-any.whl (40 kB)\n",
      "Collecting attrs==20.3.0\n",
      "  Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "Installing collected packages: attrs, grafanalib\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 19.1.0\n",
      "    Uninstalling attrs-19.1.0:\n",
      "      Successfully uninstalled attrs-19.1.0\n",
      "  Attempting uninstall: grafanalib\n",
      "    Found existing installation: grafanalib 0.5.3\n",
      "    Uninstalling grafanalib-0.5.3:\n",
      "      Successfully uninstalled grafanalib-0.5.3\n",
      "\u001B[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "grafwiz -0.1.0- requires attrs==19.1.0, but you'll have attrs 20.3.0 which is incompatible.\n",
      "grafwiz -0.1.0- requires grafanalib==0.5.3, but you'll have grafanalib 0.5.9 which is incompatible.\u001B[0m\n",
      "Successfully installed attrs-20.3.0 grafanalib-0.5.9\n"
     ]
    }
   ],
   "source": [
    "# Verify Grafwiz and the latest grafanalib are installed\n",
    "!python -m pip install git+https://github.com/v3io/grafwiz\n",
    "!python -m pip install grafanalib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasource Iguazio already exists\n",
      "Datasource Iguazio created successfully\n",
      "Datasource stream-viewer already exists\n",
      "Datasource stream-viewer created successfully\n",
      "Dashboard stocks created successfully\n"
     ]
    }
   ],
   "source": [
    "from grafwiz import *\n",
    "import v3io_frames as v3f\n",
    "\n",
    "def deploy_dashboard(grafana_url:str = 'http://grafana', \n",
    "                     streamview_url:str = 'http://nuclio-stocks-stream-viewer:8080',\n",
    "                     v3io_container:str = 'bigdata',\n",
    "                     stocks_kv_table:str = 'stocks/stocks_kv',\n",
    "                     stocks_tsdb_table:str = 'stocks/stocks_tsdb'):\n",
    "    # Create datasources\n",
    "    DataSource(name='Iguazio').deploy(grafana_url, use_auth=True)\n",
    "    DataSource(name='stream-viewer', frames_url=streamview_url).deploy(grafana_url, use_auth=False, overwrite=False)\n",
    "    \n",
    "    # Verify the KV table can be shown\n",
    "    client = v3f.Client('framesd:8081', container=v3io_container)\n",
    "    try:\n",
    "        client.execute(backend='kv', table=stocks_kv_table, command='infer')\n",
    "    except: \n",
    "        print('KV Table has no values yet, please run `client.execute(backend=\"kv\", table=stocks_kv_table, command=\"infer\")` (can be found on the 05-explore notebook.)')\n",
    "    \n",
    "    # Create grafana dashboard\n",
    "    dash = Dashboard(\"stocks\", start='now-7d', dataSource='Iguazio')\n",
    "\n",
    "    # Add a symbol combo box (template) with data from the stocks table\n",
    "    dash.template(name=\"SYMBOL\", label=\"Symbol\", query=\"fields=symbol;table=stocks/stocks_kv;backend=kv;container=bigdata\")\n",
    "\n",
    "    # Create a table and log viewer in one row\n",
    "    tbl = Table('Current Stocks Value', span=12).source(table=stocks_kv_table,fields=['symbol','volume', 'price', 'sentiment', 'last_updated'],container=v3io_container)\n",
    "    dash.row([tbl])\n",
    "\n",
    "    # Create 2 charts on the second row\n",
    "    metrics_row = [Graph(metric).series(table=stocks_tsdb_table, fields=[metric], filter='symbol==\"$SYMBOL\"',container=v3io_container) for metric in ['price','volume']]\n",
    "    metrics_row.append(Graph('sentiment').series(table=stocks_tsdb_table, fields=['sentiment'], filter='symbol==\"$SYMBOL\"', container=v3io_container))\n",
    "    dash.row(metrics_row)\n",
    "    \n",
    "    # Create log veiwer panel\n",
    "    log = Table('Articles Log', dataSource='stream-viewer', span=12)\n",
    "    dash.row([log])\n",
    "    \n",
    "    # Deploy to Grafana\n",
    "    return dash.deploy(grafana_url)\n",
    "deploy_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create deployment workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/workflow.py\n",
    "from kfp import dsl\n",
    "from mlrun import mount_v3io, mlconf\n",
    "import os\n",
    "from nuclio.triggers import V3IOStreamTrigger, CronTrigger\n",
    "\n",
    "funcs = {}\n",
    "\n",
    "# Directories and Paths\n",
    "projdir = os.path.abspath('./')\n",
    "model_filepath = os.path.join(projdir, 'models', 'model.pt') # Previously saved model if downloaded\n",
    "reviews_datafile = os.path.join(projdir, 'data', 'reviews.csv')\n",
    "\n",
    "# Performence limit\n",
    "max_replicas = 1\n",
    "\n",
    "# Readers cron interval\n",
    "readers_cron_interval = '300s'\n",
    "\n",
    "# Training GPU Allocation\n",
    "# Set to 0 if no gpus are to be used\n",
    "training_gpus = 0\n",
    "\n",
    "\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    for f in functions.values():\n",
    "        # Add V3IO Mount\n",
    "        f.apply(mount_v3io())\n",
    "        \n",
    "        # Always pull images to keep updates\n",
    "        f.spec.image_pull_policy = 'Always'\n",
    "    \n",
    "    # Define inference-stream related triggers\n",
    "    functions['sentiment_analysis_server'].add_model('bert_classifier_v1', model_filepath)\n",
    "    functions['sentiment_analysis_server'].spec.readiness_timeout = 500\n",
    "    functions['sentiment_analysis_server'].set_config('readinessTimeoutSeconds', 500)\n",
    "    \n",
    "    # Adept image to use CPU if a GPU is not assigned\n",
    "    if training_gpus == 0:\n",
    "        functions['sentiment_analysis_server'].spec.base_spec['spec']['build']['baseImage']='mlrun/ml-models'\n",
    "        functions['bert_sentiment_classifier_trainer'].spec.image='mlrun/ml-models'\n",
    "    \n",
    "    # Add triggers\n",
    "    functions['stocks_reader'].add_trigger('cron', CronTrigger(readers_cron_interval))\n",
    "    functions['news_reader'].add_trigger('cron', CronTrigger(readers_cron_interval))\n",
    "    \n",
    "    \n",
    "    # Set max replicas for resource limits\n",
    "    functions['sentiment_analysis_server'].spec.max_replicas = max_replicas\n",
    "    functions['news_reader'].spec.max_replicas = max_replicas\n",
    "    functions['stocks_reader'].spec.max_replicas = max_replicas\n",
    "    \n",
    "    # Add GPU for training\n",
    "    functions['bert_sentiment_classifier_trainer'].gpus(training_gpus)\n",
    "        \n",
    "@dsl.pipeline(\n",
    "    name='Stocks demo deployer',\n",
    "    description='Up to RT Stocks ingestion and analysis'\n",
    ")\n",
    "def kfpipeline(\n",
    "    # General\n",
    "    V3IO_CONTAINER = 'bigdata',\n",
    "    STOCKS_TSDB_TABLE = 'stocks/stocks_tsdb',\n",
    "    STOCKS_KV_TABLE = 'stocks/stocks_kv',\n",
    "    STOCKS_STREAM = 'stocks/stocks_stream',\n",
    "    RUN_TRAINER: bool = False,\n",
    "    \n",
    "    # Trainer\n",
    "    pretrained_model = 'bert-base-cased',\n",
    "    reviews_dataset = reviews_datafile,\n",
    "    models_dir = 'models',\n",
    "    model_filename = 'bert_sentiment_analysis_model.pt',\n",
    "    n_classes: int = 3,\n",
    "    MAX_LEN: int = 128,\n",
    "    BATCH_SIZE: int = 16,\n",
    "    EPOCHS: int =  2,\n",
    "    random_state: int = 42,\n",
    "    \n",
    "    # stocks reader\n",
    "    STOCK_LIST: list = ['GOOGL', 'MSFT', 'AMZN', 'AAPL', 'INTC'],\n",
    "    EXPRESSION_TEMPLATE = \"symbol='{symbol}';price={price};volume={volume};last_updated='{last_updated}'\",\n",
    "    \n",
    "    # Sentiment analysis server\n",
    "    model_name = 'bert_classifier_v1',\n",
    "    model_filepath = model_filepath # if not trained\n",
    "    \n",
    "    ):\n",
    "    \n",
    "    with dsl.Condition(RUN_TRAINER == True):\n",
    "        \n",
    "        deployer = funcs['bert_sentiment_classifier_trainer'].deploy_step()\n",
    "                \n",
    "        trainer = funcs['bert_sentiment_classifier_trainer'].as_step(name='bert_sentiment_classifier_trainer',\n",
    "                                                                     handler='train_sentiment_analysis_model',\n",
    "                                                                     params={'pretrained_model': pretrained_model,\n",
    "                                                                             'EPOCHS': EPOCHS,\n",
    "                                                                             'models_dir': models_dir,\n",
    "                                                                             'model_filename': model_filename,\n",
    "                                                                             'n_classes': n_classes,\n",
    "                                                                             'MAX_LEN': MAX_LEN,\n",
    "                                                                             'BATCH_SIZE': BATCH_SIZE,\n",
    "                                                                             'EPOCHS': EPOCHS,\n",
    "                                                                             'random_state': random_state},\n",
    "                                                                     inputs={'reviews_dataset': reviews_dataset},\n",
    "                                                                     outputs=['bert_sentiment_analysis_model'],\n",
    "                                                                     image=deployer.outputs['image'])\n",
    "        \n",
    "        sentiment_server = funcs['sentiment_analysis_server'].deploy_step(env={f'SERVING_MODEL_{model_name}': trainer.outputs['bert_sentiment_analysis_model']})\n",
    "        \n",
    "        news_reader = funcs['news_reader'].deploy_step(env={'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_STREAM': STOCKS_STREAM,\n",
    "                                                            'STOCKS_TSDB_TABLE': STOCKS_TSDB_TABLE,\n",
    "                                                            'SENTIMENT_MODEL_ENDPOINT': sentiment_server.outputs['endpoint']})\n",
    "    \n",
    "    with dsl.Condition(RUN_TRAINER == False):\n",
    "        \n",
    "        sentiment_server = funcs['sentiment_analysis_server'].deploy_step(env={f'SERVING_MODEL_{model_name}': model_filepath})\n",
    "        \n",
    "        news_reader = funcs['news_reader'].deploy_step(env={'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_STREAM': STOCKS_STREAM,\n",
    "                                                            'STOCKS_TSDB_TABLE': STOCKS_TSDB_TABLE,\n",
    "                                                            'SENTIMENT_MODEL_ENDPOINT': sentiment_server.outputs['endpoint']})\n",
    "    \n",
    "    stocks_reader = funcs['stocks_reader'].deploy_step(env={'STOCK_LIST': STOCK_LIST,\n",
    "                                                            'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_TSDB_TABLE': STOCKS_TSDB_TABLE,\n",
    "                                                            'STOCKS_KV_TABLE': STOCKS_KV_TABLE,\n",
    "                                                            'EXPRESSION_TEMPLATE': EXPRESSION_TEMPLATE})\n",
    "    \n",
    "    stream_viewer = funcs['stream_viewer'].deploy_step(env={'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_STREAM': STOCKS_STREAM}).after(news_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.set_workflow('main', os.path.join(os.path.abspath(project.context), 'code', 'workflow.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save(os.path.join(project.context, 'project.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run workflow\n",
    "In this cell we will run the `main` workflow via `KubeFlow Pipelines` on top of our cluster.  \n",
    "Running the pipeline may take some time. Due to possible jupyter timeout, it's best to track the pipeline's progress via KFP or the MLRun UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-03-16 12:51:41,311 [info] using in-cluster config.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://dashboard.default-tenant.app.dev39.lab.iguazeng.com/pipelines/#/experiments/details/0cdf9ca8-19a3-4840-87b4-483dc264f53c\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://dashboard.default-tenant.app.dev39.lab.iguazeng.com/pipelines/#/runs/details/a44f566e-37c2-4d80-980c-ddf6521579d5\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-03-16 12:51:43,603 [info] Pipeline run id=a44f566e-37c2-4d80-980c-ddf6521579d5, check UI or DB for progress\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a44f566e-37c2-4d80-980c-ddf6521579d5'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.run('main', arguments={'RUN_TRAINER': False}, artifact_path=ARTIFACT_PATH, dirty=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
=======
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stocks Analysis Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup stocks project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project name: stocks-avia\n",
      "Artifacts path: v3io:///projects/{{run.project}}/artifacts\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import mlrun\n",
    "\n",
    "# Set the base project name\n",
    "project_name_base = 'stocks'\n",
    "# Initialize the MLRun environment and save the project name and artifacts path\n",
    "project_name, artifact_path = mlrun.set_environment(project=project_name_base,\n",
    "                                                    user_project=True)\n",
    "\n",
    "project_path = path.abspath('./')\n",
    "\n",
    "project = mlrun.new_project(project_name_base,\n",
    "                            context=project_path,                           \n",
    "                            user_project=True)\n",
    "                                                    \n",
    "# Display the current project name and artifacts path\n",
    "print(f'Project name: {project_name}')\n",
    "print(f'Artifacts path: {artifact_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare project functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.serving.ServingRuntime at 0x7f5f8f9aa650>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun import mount_v3io\n",
    "# Set functions to project\n",
    "project.set_function('code/00-train-sentiment-analysis-model.ipynb', name='bert_sentiment_classifier_trainer')\n",
    "project.set_function('code/01-read-stocks.ipynb', name='stocks_reader')\n",
    "project.set_function('code/02-read-news.ipynb', name='news_reader')\n",
    "project.set_function('code/03-stream-viewer.ipynb', name='stream_viewer')\n",
    "mlrun.mlconf.hub_url = 'https://raw.githubusercontent.com/aviaIguazio/functions/development/sentiment_analysis_serving/function.yaml'\n",
    "project.set_function('hub://sentiment_analysis_serving', name='sentiment_analysis_server')\n",
    "project.func('sentiment_analysis_server').apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a pre-trained model (optional)\n",
    "Since running the [training](training/bert_sentiment_classification.ipynb) part to achieve good results may take some time, we had already trained and uploaded a model to a public location.  \n",
    "You can easily download it by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘/User/aviaIguazio/demos/stock-analysis/models/model.pt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this to download the pre-trained model to your `models` directory\n",
    "\n",
    "import os\n",
    "model_location = 'https://iguazio-sample-data.s3.amazonaws.com/models/model.pt'\n",
    "saved_models_directory = os.path.join(os.path.abspath('./'), 'models')\n",
    "\n",
    "# Create paths\n",
    "os.makedirs(saved_models_directory, exist_ok=1)\n",
    "model_filepath = os.path.join(saved_models_directory, os.path.basename(model_location))\n",
    "!wget -nc -P {saved_models_directory} {model_location} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.serving.states.TaskState at 0x7f5f8f9b8210>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add model \n",
    "project.func('sentiment_analysis_server').add_model(\"model1\", class_name='SentimentClassifierServing', model_path=model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a Grafana Dashboard  \n",
    "To track the different stocks on a live dashboard we will use **Grafana**.  <br>\n",
    "We will use [Grafwiz](https://github.com/v3io/grafwiz) to define and deploy the dashboard directly from this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/v3io/grafwiz\n",
      "  Cloning https://github.com/v3io/grafwiz to /tmp/pip-req-build-tq1moi5h\n",
      "Requirement already satisfied (use --upgrade to upgrade): grafwiz===-0.1.0- from git+https://github.com/v3io/grafwiz in /User/.pythonlibs/jupyter-avia/lib/python3.7/site-packages\n",
      "Collecting grafanalib==0.5.3\n",
      "  Using cached grafanalib-0.5.3-py3-none-any.whl (26 kB)\n",
      "Collecting attrs==19.1.0\n",
      "  Using cached attrs-19.1.0-py2.py3-none-any.whl (35 kB)\n",
      "Building wheels for collected packages: grafwiz\n",
      "  Building wheel for grafwiz (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grafwiz: filename=grafwiz-_0.1.0_-py3-none-any.whl size=9302 sha256=b782f433c7a57d46669de884cdb822b946cf88c1ef9815f50d89a4a80b5fb5ec\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-siai72jg/wheels/2f/6d/71/443942a9d87f91125fc43e7353667ca41489b2fa023fd0fc48\n",
      "Successfully built grafwiz\n",
      "Installing collected packages: attrs, grafanalib\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 20.3.0\n",
      "    Uninstalling attrs-20.3.0:\n",
      "      Successfully uninstalled attrs-20.3.0\n",
      "  Attempting uninstall: grafanalib\n",
      "    Found existing installation: grafanalib 0.5.9\n",
      "    Uninstalling grafanalib-0.5.9:\n",
      "      Successfully uninstalled grafanalib-0.5.9\n",
      "Successfully installed attrs-19.1.0 grafanalib-0.5.3\n",
      "Collecting grafanalib\n",
      "  Using cached grafanalib-0.5.9-py3-none-any.whl (40 kB)\n",
      "Collecting attrs==20.3.0\n",
      "  Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "Installing collected packages: attrs, grafanalib\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 19.1.0\n",
      "    Uninstalling attrs-19.1.0:\n",
      "      Successfully uninstalled attrs-19.1.0\n",
      "  Attempting uninstall: grafanalib\n",
      "    Found existing installation: grafanalib 0.5.3\n",
      "    Uninstalling grafanalib-0.5.3:\n",
      "      Successfully uninstalled grafanalib-0.5.3\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "grafwiz -0.1.0- requires attrs==19.1.0, but you'll have attrs 20.3.0 which is incompatible.\n",
      "grafwiz -0.1.0- requires grafanalib==0.5.3, but you'll have grafanalib 0.5.9 which is incompatible.\u001b[0m\n",
      "Successfully installed attrs-20.3.0 grafanalib-0.5.9\n"
     ]
    }
   ],
   "source": [
    "# Verify Grafwiz and the latest grafanalib are installed\n",
    "!python -m pip install git+https://github.com/v3io/grafwiz\n",
    "!python -m pip install grafanalib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasource Iguazio already exists\n",
      "Datasource Iguazio created successfully\n",
      "Datasource stream-viewer already exists\n",
      "Datasource stream-viewer created successfully\n",
      "Dashboard stocks created successfully\n"
     ]
    }
   ],
   "source": [
    "from grafwiz import *\n",
    "import v3io_frames as v3f\n",
    "\n",
    "def deploy_dashboard(grafana_url:str = 'http://grafana', \n",
    "                     streamview_url:str = 'http://nuclio-stocks-stream-viewer:8080',\n",
    "                     v3io_container:str = 'bigdata',\n",
    "                     stocks_kv_table:str = 'stocks/stocks_kv',\n",
    "                     stocks_tsdb_table:str = 'stocks/stocks_tsdb'):\n",
    "    # Create datasources\n",
    "    DataSource(name='Iguazio').deploy(grafana_url, use_auth=True)\n",
    "    DataSource(name='stream-viewer', frames_url=streamview_url).deploy(grafana_url, use_auth=False, overwrite=False)\n",
    "    \n",
    "    # Verify the KV table can be shown\n",
    "    client = v3f.Client('framesd:8081', container=v3io_container)\n",
    "    try:\n",
    "        client.execute(backend='kv', table=stocks_kv_table, command='infer')\n",
    "    except: \n",
    "        print('KV Table has no values yet, please run `client.execute(backend=\"kv\", table=stocks_kv_table, command=\"infer\")` (can be found on the 05-explore notebook.)')\n",
    "    \n",
    "    # Create grafana dashboard\n",
    "    dash = Dashboard(\"stocks\", start='now-7d', dataSource='Iguazio')\n",
    "\n",
    "    # Add a symbol combo box (template) with data from the stocks table\n",
    "    dash.template(name=\"SYMBOL\", label=\"Symbol\", query=\"fields=symbol;table=stocks/stocks_kv;backend=kv;container=bigdata\")\n",
    "\n",
    "    # Create a table and log viewer in one row\n",
    "    tbl = Table('Current Stocks Value', span=12).source(table=stocks_kv_table,fields=['symbol','volume', 'price', 'sentiment', 'last_updated'],container=v3io_container)\n",
    "    dash.row([tbl])\n",
    "\n",
    "    # Create 2 charts on the second row\n",
    "    metrics_row = [Graph(metric).series(table=stocks_tsdb_table, fields=[metric], filter='symbol==\"$SYMBOL\"',container=v3io_container) for metric in ['price','volume']]\n",
    "    metrics_row.append(Graph('sentiment').series(table=stocks_tsdb_table, fields=['sentiment'], filter='symbol==\"$SYMBOL\"', container=v3io_container))\n",
    "    dash.row(metrics_row)\n",
    "    \n",
    "    # Create log veiwer panel\n",
    "    log = Table('Articles Log', dataSource='stream-viewer', span=12)\n",
    "    dash.row([log])\n",
    "    \n",
    "    # Deploy to Grafana\n",
    "    return dash.deploy(grafana_url)\n",
    "deploy_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create deployment workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/workflow.py\n",
    "from kfp import dsl\n",
    "from mlrun import mount_v3io, mlconf\n",
    "import os\n",
    "from nuclio.triggers import V3IOStreamTrigger, CronTrigger\n",
    "\n",
    "funcs = {}\n",
    "\n",
    "# Directories and Paths\n",
    "projdir = os.path.abspath('./')\n",
    "model_filepath = os.path.join(projdir, 'models', 'model.pt') # Previously saved model if downloaded\n",
    "reviews_datafile = os.path.join(projdir, 'data', 'reviews.csv')\n",
    "\n",
    "# Performence limit\n",
    "max_replicas = 1\n",
    "\n",
    "# Readers cron interval\n",
    "readers_cron_interval = '300s'\n",
    "\n",
    "# Training GPU Allocation\n",
    "# Set to 0 if no gpus are to be used\n",
    "training_gpus = 0\n",
    "\n",
    "\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    for f in functions.values():\n",
    "        # Add V3IO Mount\n",
    "        f.apply(mount_v3io())\n",
    "        \n",
    "        # Always pull images to keep updates\n",
    "        f.spec.image_pull_policy = 'Always'\n",
    "    \n",
    "    # Define inference-stream related triggers\n",
    "    functions['sentiment_analysis_server'].add_model('bert_classifier_v1', model_filepath)\n",
    "    functions['sentiment_analysis_server'].spec.readiness_timeout = 500\n",
    "    functions['sentiment_analysis_server'].set_config('readinessTimeoutSeconds', 500)\n",
    "    \n",
    "    # Adept image to use CPU if a GPU is not assigned\n",
    "    if training_gpus == 0:\n",
    "        functions['sentiment_analysis_server'].spec.base_spec['spec']['build']['baseImage']='mlrun/ml-models'\n",
    "        functions['bert_sentiment_classifier_trainer'].spec.image='mlrun/ml-models'\n",
    "    \n",
    "    # Add triggers\n",
    "    functions['stocks_reader'].add_trigger('cron', CronTrigger(readers_cron_interval))\n",
    "    functions['news_reader'].add_trigger('cron', CronTrigger(readers_cron_interval))\n",
    "    \n",
    "    \n",
    "    # Set max replicas for resource limits\n",
    "    functions['sentiment_analysis_server'].spec.max_replicas = max_replicas\n",
    "    functions['news_reader'].spec.max_replicas = max_replicas\n",
    "    functions['stocks_reader'].spec.max_replicas = max_replicas\n",
    "    \n",
    "    # Add GPU for training\n",
    "    functions['bert_sentiment_classifier_trainer'].gpus(training_gpus)\n",
    "        \n",
    "@dsl.pipeline(\n",
    "    name='Stocks demo deployer',\n",
    "    description='Up to RT Stocks ingestion and analysis'\n",
    ")\n",
    "def kfpipeline(\n",
    "    # General\n",
    "    V3IO_CONTAINER = 'bigdata',\n",
    "    STOCKS_TSDB_TABLE = 'stocks/stocks_tsdb',\n",
    "    STOCKS_KV_TABLE = 'stocks/stocks_kv',\n",
    "    STOCKS_STREAM = 'stocks/stocks_stream',\n",
    "    RUN_TRAINER: bool = False,\n",
    "    \n",
    "    # Trainer\n",
    "    pretrained_model = 'bert-base-cased',\n",
    "    reviews_dataset = reviews_datafile,\n",
    "    models_dir = 'models',\n",
    "    model_filename = 'bert_sentiment_analysis_model.pt',\n",
    "    n_classes: int = 3,\n",
    "    MAX_LEN: int = 128,\n",
    "    BATCH_SIZE: int = 16,\n",
    "    EPOCHS: int =  2,\n",
    "    random_state: int = 42,\n",
    "    \n",
    "    # stocks reader\n",
    "    STOCK_LIST: list = ['GOOGL', 'MSFT', 'AMZN', 'AAPL', 'INTC'],\n",
    "    EXPRESSION_TEMPLATE = \"symbol='{symbol}';price={price};volume={volume};last_updated='{last_updated}'\",\n",
    "    \n",
    "    # Sentiment analysis server\n",
    "    model_name = 'bert_classifier_v1',\n",
    "    model_filepath = model_filepath # if not trained\n",
    "    \n",
    "    ):\n",
    "    \n",
    "    with dsl.Condition(RUN_TRAINER == True):\n",
    "        \n",
    "        deployer = funcs['bert_sentiment_classifier_trainer'].deploy_step()\n",
    "                \n",
    "        trainer = funcs['bert_sentiment_classifier_trainer'].as_step(name='bert_sentiment_classifier_trainer',\n",
    "                                                                     handler='train_sentiment_analysis_model',\n",
    "                                                                     params={'pretrained_model': pretrained_model,\n",
    "                                                                             'EPOCHS': EPOCHS,\n",
    "                                                                             'models_dir': models_dir,\n",
    "                                                                             'model_filename': model_filename,\n",
    "                                                                             'n_classes': n_classes,\n",
    "                                                                             'MAX_LEN': MAX_LEN,\n",
    "                                                                             'BATCH_SIZE': BATCH_SIZE,\n",
    "                                                                             'EPOCHS': EPOCHS,\n",
    "                                                                             'random_state': random_state},\n",
    "                                                                     inputs={'reviews_dataset': reviews_dataset},\n",
    "                                                                     outputs=['bert_sentiment_analysis_model'],\n",
    "                                                                     image=deployer.outputs['image'])\n",
    "        \n",
    "        sentiment_server = funcs['sentiment_analysis_server'].deploy_step(env={f'SERVING_MODEL_{model_name}': trainer.outputs['bert_sentiment_analysis_model']})\n",
    "        \n",
    "        news_reader = funcs['news_reader'].deploy_step(env={'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_STREAM': STOCKS_STREAM,\n",
    "                                                            'STOCKS_TSDB_TABLE': STOCKS_TSDB_TABLE,\n",
    "                                                            'SENTIMENT_MODEL_ENDPOINT': sentiment_server.outputs['endpoint']})\n",
    "    \n",
    "    with dsl.Condition(RUN_TRAINER == False):\n",
    "        \n",
    "        sentiment_server = funcs['sentiment_analysis_server'].deploy_step(env={f'SERVING_MODEL_{model_name}': model_filepath})\n",
    "        \n",
    "        news_reader = funcs['news_reader'].deploy_step(env={'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_STREAM': STOCKS_STREAM,\n",
    "                                                            'STOCKS_TSDB_TABLE': STOCKS_TSDB_TABLE,\n",
    "                                                            'SENTIMENT_MODEL_ENDPOINT': sentiment_server.outputs['endpoint']})\n",
    "    \n",
    "    stocks_reader = funcs['stocks_reader'].deploy_step(env={'STOCK_LIST': STOCK_LIST,\n",
    "                                                            'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_TSDB_TABLE': STOCKS_TSDB_TABLE,\n",
    "                                                            'STOCKS_KV_TABLE': STOCKS_KV_TABLE,\n",
    "                                                            'EXPRESSION_TEMPLATE': EXPRESSION_TEMPLATE})\n",
    "    \n",
    "    stream_viewer = funcs['stream_viewer'].deploy_step(env={'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_STREAM': STOCKS_STREAM}).after(news_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.set_workflow('main', os.path.join(os.path.abspath(project.context), 'code', 'workflow.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save(os.path.join(project.context, 'project.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run workflow\n",
    "In this cell we will run the `main` workflow via `KubeFlow Pipelines` on top of our cluster.  \n",
    "Running the pipeline may take some time. Due to possible jupyter timeout, it's best to track the pipeline's progress via KFP or the MLRun UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-03-17 13:12:48,612 [info] using in-cluster config.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://dashboard.default-tenant.app.dev8.lab.iguazeng.com/pipelines/#/experiments/details/de979c99-58eb-4ba5-bb10-ce26573796ed\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://dashboard.default-tenant.app.dev8.lab.iguazeng.com/pipelines/#/runs/details/319506de-656f-4da2-86f1-1b2ff64ba54f\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-03-17 13:12:49,565 [info] Pipeline run id=319506de-656f-4da2-86f1-1b2ff64ba54f, check UI or DB for progress\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'319506de-656f-4da2-86f1-1b2ff64ba54f'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.run('main', arguments={'RUN_TRAINER': False}, artifact_path=artifact_path, dirty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> upstream/0.6.x-dev
